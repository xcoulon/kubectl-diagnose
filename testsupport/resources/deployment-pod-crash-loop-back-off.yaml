# vscode-kubernetes-tools: exclude
# (because OpenShift routes seem to be interpreted as pods ¯\_(ツ)_/¯)
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  namespace: test
  name: deploy-crash-loop-back-off
spec:
  port:
    targetPort: http
  to:
    kind: Service
    name: deploy-crash-loop-back-off
    weight: 100
  wildcardPolicy: None
status:
  ingress:
  - conditions:
    - status: "True"
      type: Admitted
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: deploy-crash-loop-back-off
  namespace: test
  name: deploy-crash-loop-back-off
spec:
  ports:
  - name: http
    port: 8080
    protocol: TCP
    targetPort: http
  selector:
    app: deploy-crash-loop-back-off
status:
  loadBalancer: {}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: test
  name: deploy-crash-loop-back-off
  uid: ef401dd4-0543-48c9-8309-026be6378483
spec:
  replicas: 1
  selector:
    matchLabels:
      app: deploy-crash-loop-back-off
  template:
    metadata:
      labels:
        app: deploy-crash-loop-back-off
    spec:
      containers:
      - image: caddy:2
        imagePullPolicy: IfNotPresent
        name: default
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
        resources:
          limits:
            cpu: 500m
            memory: 100Mi
          requests:
            cpu: 100m
            memory: 20Mi
        securityContext:
          runAsNonRoot: true
        volumeMounts:
        - mountPath: /etc/caddy
          name: caddy-config
        - mountPath: /config/caddy
          name: caddy-config-cache
      securityContext:
        runAsUser: 1003870000
      serviceAccount: default
      volumes:
      - configMap:
          defaultMode: 420
          name: caddy-config-port-80
        name: caddy-config
      - emptyDir: {}
        name: caddy-config-cache
status:
  conditions:
  - message: Deployment does not have minimum availability.
    reason: MinimumReplicasUnavailable
    status: "False"
    type: Available
  - message: ReplicaSet "crash-loop-back-off-7994787459" has timed out progressing.
    reason: ProgressDeadlineExceeded
    status: "False"
    type: Progressing
  observedGeneration: 1
  replicas: 1
  unavailableReplicas: 1
  updatedReplicas: 1
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  labels:
    app: deploy-crash-loop-back-off
  namespace: test
  name: deploy-crash-loop-back-off-7994787459
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: Deployment
    name: deploy-crash-loop-back-off
    uid: ef401dd4-0543-48c9-8309-026be6378483
  uid: 750e2747-6842-4577-ad9c-4aa6f1ea75a9
spec:
  replicas: 1
  selector:
    matchLabels:
      app: deploy-crash-loop-back-off
      pod-template-hash: "7994787459"
  template:
    metadata:
      labels:
        app: deploy-crash-loop-back-off
        pod-template-hash: "7994787459"
    spec:
      containers:
      - image: caddy:2
        imagePullPolicy: IfNotPresent
        name: default
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
        resources:
          limits:
            cpu: 500m
            memory: 100Mi
          requests:
            cpu: 100m
            memory: 20Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        securityContext:
          runAsNonRoot: true
        volumeMounts:
        - mountPath: /etc/caddy
          name: caddy-config
        - mountPath: /config/caddy
          name: caddy-config-cache
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext:
        runAsUser: 1003870000
      serviceAccount: default
      volumes:
      - configMap:
          defaultMode: 420
          name: caddy-config-port-80
        name: caddy-config
      - emptyDir: {}
        name: caddy-config-cache
status:
  fullyLabeledReplicas: 1
  observedGeneration: 1
  replicas: 1
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: deploy-crash-loop-back-off
    pod-template-hash: "7994787459"
  name: deploy-crash-loop-back-off-7994787459-2nrz5
  namespace: test
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: deploy-crash-loop-back-off-7994787459
    uid: 750e2747-6842-4577-ad9c-4aa6f1ea75a9
spec:
  containers:
  - image: caddy:2
    imagePullPolicy: IfNotPresent
    name: default
    ports:
    - containerPort: 80
      name: http
      protocol: TCP
    resources:
      limits:
        cpu: 500m
        memory: 100Mi
      requests:
        cpu: 100m
        memory: 20Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      runAsUser: 1003880000
    volumeMounts:
    - mountPath: /etc/caddy
      name: caddy-config
    - mountPath: /config/caddy
      name: caddy-config-cache
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-bhzcb
      readOnly: true
  securityContext:
    fsGroup: 1003880000
    runAsNonRoot: true
    seLinuxOptions:
      level: s0:c62,c49
    seccompProfile:
      type: RuntimeDefault
  serviceAccount: default
  volumes:
  - configMap:
      defaultMode: 420
      name: caddy-config-port-80
    name: caddy-config
  - emptyDir: {}
    name: caddy-config-cache
status:
  conditions:
  - status: "True"
    type: Initialized
  - message: 'containers with unready status: [default]'
    reason: ContainersNotReady
    status: "False"
    type: Ready
  - message: 'containers with unready status: [default]'
    reason: ContainersNotReady
    status: "False"
    type: ContainersReady
  containerStatuses:
  - containerID: cri-o://2eb26dea7b2cb820fe073221637687900c426b1d535385f21899b3f29243ad3a
    image: docker.io/library/caddy:2
    imageID: docker.io/library/caddy@sha256:50743fc6130295e9e8feccd8b2f437d8c472f626bf277dc873734ed98219f44f
    name: default
    ready: false
    restartCount: 32
    started: false
    lastState:
      terminated:
        containerID: cri-o://535539c2857de6af7128c1b236193408d2d7a0560436f60591ef351a15723690
        exitCode: 1
        reason: Error
    state:
      waiting:
        message: back-off 5m0s restarting failed container=default pod=crash-loop-back-off-7994787459-2nrz5(307c7d07-dda4-4877-8195-833e2e7a9e7a)
        reason: CrashLoopBackOff
---
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: test
  name: caddy-config-port80
data:
  Caddyfile: |-
    http://:80

    respond "Hello, world!"
---
apiVersion: v1
kind: Event
metadata:
  namespace: test
  name: deploy-crash-loop-back-off.17233cf30d5bf54b
count: 4
type: Warning
reason: BackOff
message: Back-off restarting failed container
firstTimestamp: "2022-10-31T19:08:21Z"
lastTimestamp: "2022-10-31T19:08:48Z"
involvedObject:
  apiVersion: v1
  kind: Pod
  namespace: test
  name: deploy-crash-loop-back-off
---
apiVersion: v1
kind: Event
metadata:
  name: deploy-crash-loop-back-off-7994787459-2nrz5.1726e0222d5c35ed
  namespace: test
type: Warning
lastTimestamp: "2022-11-12T18:02:28Z"
message: Back-off restarting failed container
reason: BackOff
count: 691
involvedObject:
  apiVersion: v1
  fieldPath: spec.containers{default}
  kind: Pod
  namespace: test
  name: deploy-crash-loop-back-off-7994787459-2nrz5